{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Bidirectional, Dense, Embedding\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.metrics import Precision, Recall, CategoricalAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('kaggle dataset/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see that each sentence is descriped by 6 classes \n",
    "#### 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate' \n",
    "#### for each sentence 1 is added if it belongs to that class \n",
    "#### so for example the first sentence does not indicate any of the toxicity classes so 0 is added for all classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the vectorizer layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper params that can be tuned\n",
    "max_features = 100000\n",
    "max_len = 1800\n",
    "\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "              max_tokens=max_features,\n",
    "              output_mode='int',\n",
    "              output_sequence_length=max_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df['comment_text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help the vectorizer create dictionarey from the corpus\n",
    "# to be used later in vecotrization\n",
    "\n",
    "vectorize_layer.adapt(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df[df.columns[2:]].values\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MCSHBAP : map, chache, shuffle, batch, prefetch  from_tensor_slices\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((text, labels))\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(160000)\n",
    "dataset = dataset.batch(128)\n",
    "dataset = dataset.prefetch(128) # helps bottlenecks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_val_split(dataset,ratios = [0.7,0.2,0.1]):\n",
    "    \"\"\"\n",
    "    splits the dataset into train , test , validation splits\n",
    "    according to the given ratios list\n",
    "    \n",
    "    \n",
    "    args:\n",
    "        dataset : tensorflow dataset object that represents \n",
    "                  the whole dataset samples to be split\n",
    "                  \n",
    "        ratios  : a list of 3 numbers representing the percentage of samples \n",
    "                  for each split\n",
    "                  \n",
    "        returns train , test , val datasets\n",
    "        \n",
    "        example:-\n",
    "        \n",
    "        dataset = [1,2,3,4]\n",
    "        ratios = [0.5,0.25,0.25]\n",
    "        \n",
    "        print(train_test_val_split(dataset,ratios))\n",
    "        \n",
    "        >> [1,2],[3],[4]\n",
    "        \n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    n_samples = len(dataset)\n",
    "    n_train = int(ratios[0]*n_samples)\n",
    "    n_test  = int(ratios[1]*n_samples)\n",
    "    n_val   = int(ratios[2]*n_samples)\n",
    "    \n",
    "    train = dataset.take(n_train)\n",
    "    val = dataset.skip(n_train).take(n_test)\n",
    "    test = dataset.skip(n_train+n_test).take(n_val)\n",
    "    \n",
    "    return train,test,val\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset to train-test-val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_val_split(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(vectorize_layer)\n",
    "# Create the embedding layer \n",
    "model.add(Embedding(max_features+1, 32))\n",
    "# Bidirectional LSTM Layer\n",
    "model.add(Bidirectional(LSTM(32, activation='tanh')))\n",
    "# Feature extractor Fully connected layers\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "# Final layer \n",
    "model.add(Dense(6, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='BinaryCrossentropy', optimizer='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    history = model.fit(train, epochs=10, validation_data=val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "pd.DataFrame(history.history).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = 'I hate you'\n",
    "\n",
    "\n",
    "res = model.predict(np.expand_dims(input_text,0))\n",
    "\n",
    "(res > 0.5).astype(int)\n",
    "\n",
    "batch_X, batch_y = test.as_numpy_iterator().next()\n",
    "\n",
    "print((model.predict(batch_X) > 0.5).astype(int))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = Precision()\n",
    "re = Recall()\n",
    "acc = CategoricalAccuracy()\n",
    "for batch in test.as_numpy_iterator(): \n",
    "    # Unpack the batch \n",
    "    X_true, y_true = batch\n",
    "    # Make a prediction \n",
    "    yhat = model.predict(X_true)\n",
    "    \n",
    "    # Flatten the predictions\n",
    "    y_true = y_true.flatten()\n",
    "    yhat = yhat.flatten()\n",
    "    \n",
    "    pre.update_state(y_true, yhat)\n",
    "    re.update_state(y_true, yhat)\n",
    "    acc.update_state(y_true, yhat)\n",
    "print(f'Precision: {pre.result().numpy()}, Recall:{re.result().numpy()}, Accuracy:{acc.result().numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('toxicity_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
